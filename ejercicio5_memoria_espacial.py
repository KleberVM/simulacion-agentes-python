"""
EJERCICIO 5: Agente que Aprende qu√© √Åreas Tienen M√°s Comida (Memoria Espacial)
================================================================================
Implementaci√≥n de un agente con aprendizaje que construye un mapa de calor
de las √°reas donde encuentra m√°s comida, optimizando su estrategia de b√∫squeda.

Caracter√≠sticas:
- Memoria espacial: Mapa de densidad de comida por regi√≥n
- Aprendizaje por experiencia: Actualiza creencias seg√∫n hallazgos
- Exploraci√≥n vs. Explotaci√≥n: Balance entre explorar nuevas √°reas y explotar √°reas conocidas
- Toma de decisiones basada en probabilidades
"""

import random
import math
from collections import defaultdict


class MemoriaEspacial:
    """
    Estructura de datos para almacenar informaci√≥n espacial sobre el entorno.
    
    Divide el entorno en regiones y mantiene estad√≠sticas sobre cada una:
    - Visitas: Cu√°ntas veces visit√≥ la regi√≥n
    - Comida encontrada: Cantidad de comida hallada en la regi√≥n
    - Densidad: Comida por visita (indicador de productividad)
    
    Atributos:
        tamano_region: Tama√±o de cada regi√≥n (en celdas)
        regiones: Dict {(rx, ry): {'visitas': int, 'comida': int, 'densidad': float}}
    """
    
    def __init__(self, tamano_region=3):
        self.tamano_region = tamano_region
        self.regiones = defaultdict(lambda: {'visitas': 0, 'comida': 0, 'densidad': 0.0})
    
    def obtener_region(self, x, y):
        """
        Convierte coordenadas del mundo a coordenadas de regi√≥n.
        
        Args:
            x, y: Coordenadas en el mundo
            
        Returns:
            tuple: (rx, ry) coordenadas de la regi√≥n
        """
        rx = x // self.tamano_region
        ry = y // self.tamano_region
        return (rx, ry)
    
    def registrar_visita(self, x, y, encontro_comida=False):
        """
        Registra una visita a una posici√≥n y actualiza estad√≠sticas.
        
        Args:
            x, y: Coordenadas visitadas
            encontro_comida: Si encontr√≥ comida en esa posici√≥n
        """
        region = self.obtener_region(x, y)
        self.regiones[region]['visitas'] += 1
        
        if encontro_comida:
            self.regiones[region]['comida'] += 1
        
        # Actualizar densidad (comida por visita)
        visitas = self.regiones[region]['visitas']
        comida = self.regiones[region]['comida']
        self.regiones[region]['densidad'] = comida / visitas if visitas > 0 else 0.0
    
    def obtener_densidad(self, x, y):
        """
        Obtiene la densidad de comida de una regi√≥n.
        
        Returns:
            float: Densidad de comida (0.0 a 1.0+)
        """
        region = self.obtener_region(x, y)
        return self.regiones[region]['densidad']
    
    def obtener_mejor_region(self):
        """
        Encuentra la regi√≥n con mayor densidad de comida.
        
        Returns:
            tuple: (rx, ry) de la mejor regi√≥n, o None si no hay datos
        """
        if not self.regiones:
            return None
        
        mejor_region = max(
            self.regiones.items(),
            key=lambda item: item[1]['densidad']
        )
        
        return mejor_region[0] if mejor_region[1]['densidad'] > 0 else None
    
    def obtener_estadisticas(self):
        """Retorna un resumen de las estad√≠sticas de memoria."""
        if not self.regiones:
            return "Sin datos"
        
        total_visitas = sum(r['visitas'] for r in self.regiones.values())
        total_comida = sum(r['comida'] for r in self.regiones.values())
        regiones_exploradas = len(self.regiones)
        
        return {
            'regiones_exploradas': regiones_exploradas,
            'total_visitas': total_visitas,
            'total_comida': total_comida,
            'densidad_promedio': total_comida / total_visitas if total_visitas > 0 else 0
        }


class AgenteConAprendizaje:
    """
    Agente que aprende sobre la distribuci√≥n de comida en el entorno.
    
    Estrategia:
    - Explora el entorno y construye un mapa mental
    - Aprende qu√© regiones son m√°s productivas
    - Balancea exploraci√≥n (buscar nuevas √°reas) con explotaci√≥n (ir a √°reas conocidas)
    
    Atributos:
        x, y: Posici√≥n actual
        entorno: Referencia al entorno
        memoria: Objeto MemoriaEspacial
        comida_recolectada: Contador de comida recolectada
        epsilon: Probabilidad de exploraci√≥n (vs. explotaci√≥n)
        pasos_totales: Contador de pasos dados
    """
    
    def __init__(self, x, y, entorno, tamano_region=3):
        self.x = x
        self.y = y
        self.entorno = entorno
        self.memoria = MemoriaEspacial(tamano_region)
        self.comida_recolectada = 0
        self.epsilon = 0.3  # 30% exploraci√≥n, 70% explotaci√≥n
        self.pasos_totales = 0
        self.objetivo_actual = None
    
    def percibir(self):
        """
        Percibe si hay comida en la posici√≥n actual.
        
        Returns:
            bool: True si hay comida
        """
        return self.entorno.hay_comida(self.x, self.y)
    
    def decidir_estrategia(self):
        """
        Decide entre exploraci√≥n y explotaci√≥n usando epsilon-greedy.
        
        Returns:
            str: 'explorar' o 'explotar'
        """
        if random.random() < self.epsilon:
            return 'explorar'
        else:
            return 'explotar'
    
    def seleccionar_objetivo_explotacion(self):
        """
        Selecciona un objetivo en la regi√≥n con mayor densidad de comida.
        
        Returns:
            tuple: (x, y) objetivo en regi√≥n productiva
        """
        mejor_region = self.memoria.obtener_mejor_region()
        
        if mejor_region is None:
            # Si no hay datos, explorar
            return self.seleccionar_objetivo_exploracion()
        
        # Convertir regi√≥n a coordenadas del mundo
        rx, ry = mejor_region
        tamano = self.memoria.tamano_region
        
        # Seleccionar punto aleatorio dentro de esa regi√≥n
        x = rx * tamano + random.randint(0, tamano - 1)
        y = ry * tamano + random.randint(0, tamano - 1)
        
        # Ajustar a l√≠mites del entorno
        x = max(0, min(x, self.entorno.ancho - 1))
        y = max(0, min(y, self.entorno.alto - 1))
        
        return (x, y)
    
    def seleccionar_objetivo_exploracion(self):
        """
        Selecciona un objetivo aleatorio para exploraci√≥n.
        
        Returns:
            tuple: (x, y) objetivo aleatorio
        """
        x = random.randint(0, self.entorno.ancho - 1)
        y = random.randint(0, self.entorno.alto - 1)
        return (x, y)
    
    def decidir_y_actuar(self):
        """
        Ciclo completo de decisi√≥n y acci√≥n del agente.
        """
        # 1. Percibir entorno actual
        hay_comida = self.percibir()
        
        # 2. Registrar visita en memoria
        self.memoria.registrar_visita(self.x, self.y, hay_comida)
        
        # 3. Recolectar comida si hay
        if hay_comida:
            if self.entorno.recolectar_comida(self.x, self.y):
                self.comida_recolectada += 1
                self.objetivo_actual = None  # Buscar nuevo objetivo
        
        # 4. Decidir nuevo objetivo si no tiene uno
        if self.objetivo_actual is None or (self.x, self.y) == self.objetivo_actual:
            estrategia = self.decidir_estrategia()
            
            if estrategia == 'explotar':
                self.objetivo_actual = self.seleccionar_objetivo_explotacion()
            else:
                self.objetivo_actual = self.seleccionar_objetivo_exploracion()
        
        # 5. Moverse hacia el objetivo
        self.mover_hacia_objetivo()
        self.pasos_totales += 1
        
        # 6. Reducir epsilon con el tiempo (menos exploraci√≥n, m√°s explotaci√≥n)
        # Decae exponencialmente: epsilon = epsilon_inicial * 0.995^pasos
        self.epsilon = max(0.1, self.epsilon * 0.995)
    
    def mover_hacia_objetivo(self):
        """Mueve el agente un paso hacia su objetivo actual."""
        if self.objetivo_actual is None:
            return
        
        dx = 0
        dy = 0
        
        if self.objetivo_actual[0] > self.x:
            dx = 1
        elif self.objetivo_actual[0] < self.x:
            dx = -1
        
        if self.objetivo_actual[1] > self.y:
            dy = 1
        elif self.objetivo_actual[1] < self.y:
            dy = -1
        
        # Priorizar movimiento en X o Y aleatoriamente
        if dx != 0 and dy != 0:
            if random.random() < 0.5:
                self.x += dx
            else:
                self.y += dy
        elif dx != 0:
            self.x += dx
        elif dy != 0:
            self.y += dy
        
        # Asegurar que est√© dentro de l√≠mites
        self.x = max(0, min(self.x, self.entorno.ancho - 1))
        self.y = max(0, min(self.y, self.entorno.alto - 1))


class EntornoConDistribucionComida:
    """
    Entorno donde la comida se distribuye en clusters (√°reas concentradas).
    
    Esto simula un entorno realista donde los recursos no est√°n uniformemente
    distribuidos, sino que se concentran en ciertas √°reas.
    
    Atributos:
        ancho, alto: Dimensiones del grid
        comida: Set de tuplas (x, y) con posiciones de comida
        comida_inicial: Cantidad inicial de comida (para estad√≠sticas)
    """

    def __init__(self, ancho, alto, num_clusters=4, comida_por_cluster=8):
        self.ancho = ancho
        self.alto = alto
        self.comida = set()
        
        # Generar clusters de comida
        for _ in range(num_clusters):
            # Centro del cluster
            cx = random.randint(2, ancho - 3)
            cy = random.randint(2, alto - 3)
            
            # Generar comida alrededor del centro
            for _ in range(comida_por_cluster):
                # Distribuci√≥n normal alrededor del centro
                offset_x = int(random.gauss(0, 2))
                offset_y = int(random.gauss(0, 2))
                
                x = max(0, min(cx + offset_x, ancho - 1))
                y = max(0, min(cy + offset_y, alto - 1))
                
                self.comida.add((x, y))
        
        self.comida_inicial = len(self.comida)

    def hay_comida(self, x, y):
        """Verifica si hay comida en la posici√≥n."""
        return (x, y) in self.comida

    def recolectar_comida(self, x, y):
        """
        Recolecta comida si existe.
        
        Returns:
            bool: True si hab√≠a comida y fue recolectada
        """
        if (x, y) in self.comida:
            self.comida.remove((x, y))
            return True
        return False

    def mostrar(self, agente):
        """
        Visualizaci√≥n del entorno con mapa de calor de memoria.
        """
        for y in range(self.alto):
            fila = ""
            for x in range(self.ancho):
                if x == agente.x and y == agente.y:
                    fila += "ü§ñ "
                elif (x, y) in self.comida:
                    fila += "üçé "
                else:
                    # Mostrar densidad aprendida con intensidad de color
                    densidad = agente.memoria.obtener_densidad(x, y)
                    if densidad > 0.5:
                        fila += "üü• "  # Alta densidad
                    elif densidad > 0.3:
                        fila += "üüß "  # Media-alta densidad
                    elif densidad > 0.1:
                        fila += "üü® "  # Media densidad
                    elif densidad > 0:
                        fila += "‚¨úÔ∏è "  # Baja densidad
                    else:
                        fila += "‚¨õÔ∏è "  # No visitado
            print(fila)
        print()


# ============================================================================
# SIMULACI√ìN
# ============================================================================

def simular_agente_con_aprendizaje(pasos=80):
    """
    Ejecuta la simulaci√≥n del agente con memoria espacial.
    
    Args:
        pasos: N√∫mero m√°ximo de pasos de simulaci√≥n
    """
    # Crear entorno con comida en clusters
    entorno = EntornoConDistribucionComida(15, 12, num_clusters=5, comida_por_cluster=10)
    
    # Crear agente
    agente = AgenteConAprendizaje(7, 6, entorno, tamano_region=3)
    
    print("=" * 80)
    print("EJERCICIO 5: AGENTE CON MEMORIA ESPACIAL Y APRENDIZAJE")
    print("=" * 80)
    print("\nConcepto: El agente aprende qu√© √°reas tienen m√°s comida")
    print("Estrategia: Epsilon-greedy (exploraci√≥n vs. explotaci√≥n)")
    print("Memoria: Mapa de densidad de comida por regi√≥n\n")
    print("Leyenda del mapa de calor:")
    print("  üü• = Alta densidad de comida aprendida")
    print("  üüß = Media-alta densidad")
    print("  üü® = Media densidad")
    print("  ‚¨úÔ∏è = Baja densidad")
    print("  ‚¨õÔ∏è = √Årea no explorada\n")
    print("Estado inicial:")
    entorno.mostrar(agente)
    print(f"Comida total: {len(entorno.comida)}")
    print(f"Epsilon inicial: {agente.epsilon:.2f}\n")

    for paso in range(pasos):
        # El agente ejecuta su ciclo
        agente.decidir_y_actuar()
        
        # Mostrar estado cada 15 pasos
        if (paso + 1) % 15 == 0:
            print(f"\n{'='*80}")
            print(f"PASO {paso + 1}")
            print('='*80)
            entorno.mostrar(agente)
            
            stats = agente.memoria.obtener_estadisticas()
            print(f"Posici√≥n: ({agente.x}, {agente.y})")
            print(f"Comida recolectada: {agente.comida_recolectada}/{entorno.comida_inicial}")
            print(f"Comida restante: {len(entorno.comida)}")
            print(f"Epsilon actual: {agente.epsilon:.3f} (exploraci√≥n)")
            print(f"\nMemoria espacial:")
            print(f"  - Regiones exploradas: {stats['regiones_exploradas']}")
            print(f"  - Total visitas: {stats['total_visitas']}")
            print(f"  - Densidad promedio: {stats['densidad_promedio']:.3f}")
            
            mejor_region = agente.memoria.obtener_mejor_region()
            if mejor_region:
                densidad = agente.memoria.regiones[mejor_region]['densidad']
                print(f"  - Mejor regi√≥n: {mejor_region} (densidad: {densidad:.3f})")
        
        # Condici√≥n de salida
        if len(entorno.comida) == 0:
            print(f"\n{'='*80}")
            print("¬°√âXITO! Toda la comida ha sido recolectada")
            print('='*80)
            break
    
    # Reporte final
    print(f"\n{'='*80}")
    print("REPORTE FINAL")
    print('='*80)
    entorno.mostrar(agente)
    
    stats = agente.memoria.obtener_estadisticas()
    
    print(f"\nüìä Estad√≠sticas de recolecci√≥n:")
    print(f"  ‚úÖ Comida recolectada: {agente.comida_recolectada}/{entorno.comida_inicial}")
    print(f"  ‚ùå Comida restante: {len(entorno.comida)}")
    print(f"  üë£ Pasos totales: {agente.pasos_totales}")
    print(f"  ‚ö° Eficiencia: {agente.comida_recolectada / agente.pasos_totales:.3f} comida/paso")
    
    print(f"\nüß† Estad√≠sticas de aprendizaje:")
    print(f"  üìç Regiones exploradas: {stats['regiones_exploradas']}")
    print(f"  üîç Total de visitas: {stats['total_visitas']}")
    print(f"  üìà Densidad promedio aprendida: {stats['densidad_promedio']:.3f}")
    print(f"  üéØ Epsilon final: {agente.epsilon:.3f}")
    
    # An√°lisis de aprendizaje
    print(f"\nüí° An√°lisis:")
    if agente.comida_recolectada / entorno.comida_inicial > 0.8:
        print("  - Excelente desempe√±o en recolecci√≥n")
    elif agente.comida_recolectada / entorno.comida_inicial > 0.6:
        print("  - Buen desempe√±o en recolecci√≥n")
    else:
        print("  - Desempe√±o mejorable")
    
    if stats['densidad_promedio'] > 0.15:
        print("  - Aprendizaje efectivo: identific√≥ √°reas productivas")
    else:
        print("  - Aprendizaje en progreso: necesita m√°s exploraci√≥n")


# ============================================================================
# EJECUCI√ìN
# ============================================================================

if __name__ == "__main__":
    simular_agente_con_aprendizaje()
